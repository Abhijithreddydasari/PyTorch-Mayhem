{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()  # Python 2.0 & above applicable\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim =  embed_size // heads\n",
    "\n",
    "        assert(self.head_dim * heads == embed_size)\n",
    "\n",
    "        # Where are we using these ones in the `forward function`?\n",
    "        self.queries = nn.Linear(self.embed_size, self.embed_size, bias=False)\n",
    "        self.keys = nn.Linear(self.embed_size, self.embed_size, bias=False)\n",
    "        self.values = nn.Linear(self.embed_size, self.embed_size, bias=False)\n",
    "\n",
    "        self.fc_out = nn.Linear(self.heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, query, keys, values, mask: bool):\n",
    "        N = query.shape[0]  # Training batch size or number of batches?\n",
    "        query_len, key_len, value_len = query.shape[1], keys.shape[1], values.shape[1]\n",
    "\n",
    "        queries_proj = self.queries(query)  # (N, Seq, embed_size)\n",
    "        keys_proj = self.keys(keys)\n",
    "        values_proj = self.values(values)\n",
    "\n",
    "        # Split embedding into self.heads pieces\n",
    "        queries = queries_proj.reshape(N, query_len, self.heads, self.head_dim)  # What is the difference between key_len and N in the first place?\n",
    "        keys = keys_proj.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        values = values_proj.reshape(N, value_len, self.heads, self.head_dim)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd, nkhd->nhqk\", [queries, keys])  # Why an array? & Why this arrangement?\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        \n",
    "        attention = torch.softmax(energy/ self.embed_size ** (1/2), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhqk,nkhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )  # The shape of the whole must be looked at & how does it reshape this (are the vectors going to the correct place)??\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        \"\"\"\n",
    "        The forward expansion term basically takes the vector from a lower dimensional space to a \n",
    "        higher dimensional space for better representation capacity. The value used in the paper is\n",
    "        equal to 4.\n",
    "        \"\"\"\n",
    "        super().__init__()  # Python 3.0 & above applicable\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)  # Why embedding size?\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        attention = self.attention(query, key, value, mask) # How is this calling the forward method?\n",
    "\n",
    "        x = self.dropout(self.norm1(attention + query))  # Adding Skip Connections\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(  # Why isn't any input in the initialization? How is the input fed to the Encoder?\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length  # Maximum possible length of the sentence\n",
    "    ):  # The above parameters are hyperparameters for the model that we are training\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size  # Do we really need another instance variable embed_size??\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)  # Is this good enough for big models?\n",
    "        self.positional_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_size, heads, dropout, forward_expansion) for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)  # Why do we need this again? We don't seem to be using this in `forward`\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)  # What does .expand() do here?\n",
    "        out = self.word_embedding(x) + self.positional_embedding(positions)  # Adding or concatenation?\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)  # What is the use of having separate matrices defined when we pass the same thing here?\n",
    "            # Also why this? Can't we directly use the TransformerBlock instead of self.layers?\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, target_mask):  # Read about src and target masks\n",
    "        attention = self.attention(x, x, x, target_mask)\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(query, key, value, src_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(target_vocab_size, embed_size)  # If we get the value embeddings from the previous layer, do we need this? (for seq2seq) - is my question sensible in the first place?\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device) for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, target_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, target_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))  # Why dropout here?\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, target_mask)  # Why are we giving the same values here when we have separate matrices for keys and values?\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        target_vocab_size,\n",
    "        src_pad_idx,  # Necessary to compute mask we are going to use\n",
    "        target_pad_idx,\n",
    "        embed_size = 256,\n",
    "        num_layers = 6,\n",
    "        forward_expansion = 4,\n",
    "        heads = 8,\n",
    "        dropout = 0,\n",
    "        device = \"cuda\",\n",
    "        max_length = 100\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            target_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length  # What is the purpose of this variable??\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.target_pad_idx = target_pad_idx\n",
    "        self.device =  device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)  # Look into the .unsqueeze() function\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_target_mask(self, target):\n",
    "        N, target_len = target.shape\n",
    "        target_mask = torch.tril(torch.ones((target_len, target_len))).expand(  # Triangular lower for masking\n",
    "            N, 1, target_len, target_len  # To expand it to the training examples I guess?\n",
    "        )\n",
    "        return target_mask.to(self.device)  # Don't forget to return AND move to device!\n",
    "\n",
    "    def forward(self, src, target):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        target_mask = self.make_target_mask(target)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(target, enc_src, src_mask, target_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "tensor([[[-0.5408,  0.3239, -0.7158,  0.2044, -1.6625,  0.4420,  0.7030,\n",
      "          -0.2096, -0.7952, -0.2962],\n",
      "         [-0.1051,  0.0705, -0.3269,  0.2955,  0.2590,  0.8724, -0.1583,\n",
      "          -0.6905, -0.4783,  0.2919],\n",
      "         [-0.2632,  1.1045,  0.4545, -0.3623, -0.8390,  1.5524,  0.2520,\n",
      "          -0.7388,  0.2325,  0.8423],\n",
      "         [-1.2933,  0.0230, -0.0591,  0.7313, -1.0477,  0.7772, -0.5674,\n",
      "          -1.0164, -1.0040,  0.7085],\n",
      "         [-0.0625,  0.0576, -0.4781,  0.3183, -0.6959, -0.4410, -0.7745,\n",
      "          -0.6779,  0.2478, -0.8871],\n",
      "         [ 0.4092,  0.2617, -0.3576, -0.0663, -0.4656,  0.3951, -0.7548,\n",
      "          -0.4140,  0.2158,  0.1852],\n",
      "         [-0.7933,  0.3552,  0.1669, -0.6749, -0.8392,  0.4564,  0.2412,\n",
      "           0.3939,  0.2433,  0.0156]],\n",
      "\n",
      "        [[-0.6584,  0.4709, -0.8047,  0.3188, -1.6987,  0.3926,  0.8262,\n",
      "          -0.3585, -0.8416, -0.2737],\n",
      "         [-0.0743,  0.2823, -0.4805,  0.2492, -0.8091,  0.4418, -0.4845,\n",
      "          -0.7410, -0.1551,  0.0787],\n",
      "         [-0.4723,  0.8726,  0.8263,  0.6097, -1.0653,  1.1052,  0.0570,\n",
      "          -0.1336,  0.1977,  0.3017],\n",
      "         [-0.6563,  0.9490,  0.2744,  0.6337, -0.8535,  0.3226,  0.0962,\n",
      "          -0.1455, -0.5326,  0.8532],\n",
      "         [-0.4848,  0.9090,  0.4444, -0.0183, -0.8592,  0.1803,  0.1314,\n",
      "          -0.9397,  0.1331,  0.5298],\n",
      "         [ 0.1352,  0.3922, -0.3537,  0.4683,  0.4401, -0.0103, -0.5389,\n",
      "          -0.9164, -0.2898, -0.1986],\n",
      "         [-0.6728,  0.3869,  0.3598,  0.5309, -1.0075,  0.7838, -0.0711,\n",
      "           0.6421,  0.2347,  0.0955]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([2, 7, 10])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(\n",
    "    device\n",
    ")\n",
    "trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
    "\n",
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "src_vocab_size = 10\n",
    "trg_vocab_size = 10\n",
    "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(\n",
    "    device\n",
    ")\n",
    "out = model(x, trg[:, :-1])\n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "We're using **SAMSum** — a dialogue summarization dataset with chat conversations and their summaries.\n",
    "\n",
    "**What's in SAMSum?**\n",
    "- `dialogue`: The chat conversation (what we feed to the **encoder**)\n",
    "- `summary`: Short summary of the conversation (what we train the **decoder** to produce)\n",
    "- `id`: Unique identifier\n",
    "\n",
    "We'll take the first 2000 samples to keep training fast while learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2000 samples\n",
      "Features: {'id': Value('string'), 'dialogue': Value('string'), 'summary': Value('string')}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load SAMSum dataset (~2.5 MB download)\n",
    "# We only load the 'train' split and take first 2000 samples\n",
    "dataset = load_dataset(\"knkarthick/samsum\", split=\"train[:2000]\")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} samples\")\n",
    "print(f\"Features: {dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DIALOGUE (Input to Encoder):\n",
      "============================================================\n",
      "Amanda: I baked  cookies. Do you want some?\n",
      "Jerry: Sure!\n",
      "Amanda: I'll bring you tomorrow :-)\n",
      "\n",
      "[Length: 92 characters]\n",
      "\n",
      "============================================================\n",
      "SUMMARY (Target for Decoder):\n",
      "============================================================\n",
      "Amanda baked cookies and will bring Jerry some tomorrow.\n",
      "\n",
      "[Length: 56 characters]\n"
     ]
    }
   ],
   "source": [
    "# Let's look at ONE example to understand what we're working with\n",
    "sample = dataset[0]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DIALOGUE (Input to Encoder):\")\n",
    "print(\"=\" * 60)\n",
    "print(sample['dialogue'])\n",
    "print(f\"\\n[Length: {len(sample['dialogue'])} characters]\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY (Target for Decoder):\")\n",
    "print(\"=\" * 60)\n",
    "print(sample['summary'])\n",
    "print(f\"\\n[Length: {len(sample['summary'])} characters]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIALOGUE lengths (in words):\n",
      "  Min: 7, Max: 471, Avg: 95\n",
      "\n",
      "SUMMARY lengths (in words):\n",
      "  Min: 1, Max: 60, Avg: 21\n"
     ]
    }
   ],
   "source": [
    "# Let's understand the length distribution - this helps us set max_length later\n",
    "dialogue_lengths = [len(d.split()) for d in dataset['dialogue']]  # Word count\n",
    "summary_lengths = [len(s.split()) for s in dataset['summary']]\n",
    "\n",
    "print(\"DIALOGUE lengths (in words):\")\n",
    "print(f\"  Min: {min(dialogue_lengths)}, Max: {max(dialogue_lengths)}, Avg: {sum(dialogue_lengths)/len(dialogue_lengths):.0f}\")\n",
    "\n",
    "print(\"\\nSUMMARY lengths (in words):\")\n",
    "print(f\"  Min: {min(summary_lengths)}, Max: {max(summary_lengths)}, Avg: {sum(summary_lengths)/len(summary_lengths):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding\n",
    "\n",
    "The BPE algorithm operates by *iteratively* replacing the most frequent pair of bytes (or characters) in a dataset with a new token. This process continues until a predefined vocabulary size is reached or no more pairs can be merged. This tokenization method allows you to more densely represent the incoming text data, allowing more information to be captured in fewer tokens (and thus fit into a fixed finite-sized context window), at the expense of larger vocabulary sizes.\n",
    "\n",
    "**Properties:**\n",
    "- **Reversible and lossless**, so you can convert tokens back into the original text\n",
    "- **Works on arbitrary text**, even text that is not in the tokeniser's training data\n",
    "- **Compresses the text:** the token sequence is shorter than the bytes corresponding to the original text. On average, in practice, each token corresponds to about 4 bytes.\n",
    "- **Attempts to let the model see common subwords.** For instance, \"ing\" is a common subword in English, so BPE encodings will often split \"encoding\" into tokens like \"encod\" and \"ing\" (instead of e.g. \"enc\" and \"oding\"). Because the model will then see the \"ing\" token again and again in different contexts, it helps models generalise and better understand grammar.\n",
    "\n",
    "**Benefits:**\n",
    "- Fixed vocabulary size (no infinite words)\n",
    "- Handles rare/new words by breaking them down\n",
    "- Balances between character-level and word-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n",
      "Special tokens: pad='<|endoftext|>', eos='<|endoftext|>', bos='<|endoftext|>'\n",
      "Pad token ID: 50256\n"
     ]
    }
   ],
   "source": [
    "# Install tiktoken if needed: !pip install tiktoken\n",
    "import tiktoken\n",
    "\n",
    "# Load GPT-2's BPE tokenizer using tiktoken (OpenAI's official tokenizer)\n",
    "# This bypasses the buggy transformers library code\n",
    "# Has only one special token: <|endoftext|>\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Create a simple wrapper class to make tiktoken compatible with our code\n",
    "class TiktokenWrapper:\n",
    "    def __init__(self, encoding):\n",
    "        self.encoding = encoding\n",
    "        self.pad_token_id = encoding.eot_token  # End of text token as pad\n",
    "        self.eos_token_id = encoding.eot_token\n",
    "        self.bos_token_id = encoding.eot_token  # GPT-2 uses same token\n",
    "        self.pad_token = \"<|endoftext|>\"\n",
    "        self.eos_token = \"<|endoftext|>\"\n",
    "        self.bos_token = \"<|endoftext|>\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.encoding.n_vocab\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return self.encoding.encode(text)\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        return self.encoding.decode(token_ids)\n",
    "    \n",
    "    def convert_ids_to_tokens(self, token_ids):\n",
    "        return [self.encoding.decode([tid]) for tid in token_ids]\n",
    "    \n",
    "    def __call__(self, text, max_length=None, padding=None, truncation=None, return_tensors=None):\n",
    "        \"\"\"Tokenize with padding/truncation support\"\"\"\n",
    "        import torch\n",
    "        \n",
    "        token_ids = self.encoding.encode(text)\n",
    "        \n",
    "        # Truncate if needed\n",
    "        if truncation and max_length and len(token_ids) > max_length:\n",
    "            token_ids = token_ids[:max_length]\n",
    "        \n",
    "        # Pad if needed\n",
    "        if padding == 'max_length' and max_length:\n",
    "            pad_length = max_length - len(token_ids)\n",
    "            token_ids = token_ids + [self.pad_token_id] * pad_length\n",
    "        \n",
    "        result = {'input_ids': token_ids}\n",
    "        \n",
    "        if return_tensors == 'pt':\n",
    "            result['input_ids'] = torch.tensor([result['input_ids']])\n",
    "        \n",
    "        return result\n",
    "\n",
    "tokenizer = TiktokenWrapper(enc)\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Special tokens: pad={tokenizer.pad_token!r}, eos={tokenizer.eos_token!r}, bos={tokenizer.bos_token!r}\")\n",
    "print(f\"Pad token ID: {tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Amanda baked cookies and will bring Jerry some tomorrow.\n",
      "Token IDs: [5840, 5282, 22979, 14746, 290, 481, 2222, 13075, 617, 9439, 13]\n",
      "Number of tokens: 11\n",
      "Decoded back: Amanda baked cookies and will bring Jerry some tomorrow.\n",
      "\n",
      "Subword tokens: ['Am', 'anda', ' baked', ' cookies', ' and', ' will', ' bring', ' Jerry', ' some', ' tomorrow', '.']\n",
      "(Ġ means 'starts with a space' in GPT-2's encoding)\n"
     ]
    }
   ],
   "source": [
    "# Let's see BPE in action!\n",
    "# Notice how it breaks words into subword units\n",
    "\n",
    "test_text = \"Amanda baked cookies and will bring Jerry some tomorrow.\"\n",
    "\n",
    "# Tokenize: text → token IDs\n",
    "token_ids = tokenizer.encode(test_text)\n",
    "print(f\"Original text: {test_text}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Number of tokens: {len(token_ids)}\")\n",
    "\n",
    "# Decode: token IDs → text (to verify)\n",
    "decoded = tokenizer.decode(token_ids)\n",
    "print(f\"Decoded back: {decoded}\")\n",
    "\n",
    "# See the actual subword tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(f\"\\nSubword tokens: {tokens}\")\n",
    "print(\"(Ġ means 'starts with a space' in GPT-2's encoding)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIALOGUE token lengths (after BPE):\n",
      "  Min: 15, Max: 679, Avg: 150\n",
      "\n",
      "SUMMARY token lengths (after BPE):\n",
      "  Min: 1, Max: 73, Avg: 26\n",
      "\n",
      "DIALOGUE 95th percentile: 367 tokens\n",
      "SUMMARY 95th percentile: 54 tokens\n",
      "Length of datset:  2000\n"
     ]
    }
   ],
   "source": [
    "# Now let's see how our dataset looks when tokenized\n",
    "# This helps us decide max_length for the model\n",
    "\n",
    "dialogue_token_lengths = [len(tokenizer.encode(d)) for d in dataset['dialogue']]\n",
    "summary_token_lengths = [len(tokenizer.encode(s)) for s in dataset['summary']]\n",
    "\n",
    "print(\"DIALOGUE token lengths (after BPE):\")\n",
    "print(f\"  Min: {min(dialogue_token_lengths)}, Max: {max(dialogue_token_lengths)}, Avg: {sum(dialogue_token_lengths)/len(dialogue_token_lengths):.0f}\")\n",
    "\n",
    "print(\"\\nSUMMARY token lengths (after BPE):\")\n",
    "print(f\"  Min: {min(summary_token_lengths)}, Max: {max(summary_token_lengths)}, Avg: {sum(summary_token_lengths)/len(summary_token_lengths):.0f}\")\n",
    "\n",
    "# Percentiles help us choose max_length\n",
    "import numpy as np\n",
    "print(f\"\\nDIALOGUE 95th percentile: {np.percentile(dialogue_token_lengths, 95):.0f} tokens\")\n",
    "print(f\"SUMMARY 95th percentile: {np.percentile(summary_token_lengths, 95):.0f} tokens\")\n",
    "print('Length of datset: ', len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting max_length\n",
    "\n",
    "Based on the 95th percentile, we'll set:\n",
    "- **Source (dialogue) max_length**: 512 tokens (covers 95%+ of samples)\n",
    "- **Target (summary) max_length**: 64 tokens\n",
    "\n",
    "Sequences longer than this will be **truncated**, shorter ones will be **padded**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source shape: torch.Size([512])\n",
      "Target shape: torch.Size([64])\n",
      "\n",
      "First 20 source tokens: [5840, 5282, 25, 314, 22979, 220, 14746, 13, 2141, 345, 765, 617, 30, 198, 43462, 25, 10889, 0, 198, 5840]\n",
      "First 20 target tokens: [5840, 5282, 22979, 14746, 290, 481, 2222, 13075, 617, 9439, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for tokenization\n",
    "SRC_MAX_LENGTH = 512  # Max tokens for dialogue (encoder input)\n",
    "TRG_MAX_LENGTH = 64   # Max tokens for summary (decoder input/output)\n",
    "\n",
    "def tokenize_sample(dialogue, summary):\n",
    "    \"\"\"\n",
    "    Tokenize a single dialogue-summary pair.\n",
    "    \n",
    "    Returns:\n",
    "        src_ids: Token IDs for dialogue (encoder input)\n",
    "        trg_ids: Token IDs for summary (decoder input/output)\n",
    "    \"\"\"\n",
    "    # Tokenize with padding and truncation\n",
    "    src_encoded = tokenizer(\n",
    "        dialogue,\n",
    "        max_length=SRC_MAX_LENGTH,\n",
    "        padding='max_length',      # Pad to max_length\n",
    "        truncation=True,           # Truncate if longer\n",
    "        return_tensors='pt'        # Return PyTorch tensors\n",
    "    )\n",
    "    \n",
    "    trg_encoded = tokenizer(\n",
    "        summary,\n",
    "        max_length=TRG_MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return src_encoded['input_ids'].squeeze(), trg_encoded['input_ids'].squeeze()\n",
    "\n",
    "# Test on one sample\n",
    "src_ids, trg_ids = tokenize_sample(dataset[0]['dialogue'], dataset[0]['summary'])\n",
    "print(f\"Source shape: {src_ids.shape}\")  # Should be [512]\n",
    "print(f\"Target shape: {trg_ids.shape}\")  # Should be [64]\n",
    "print(f\"\\nFirst 20 source tokens: {src_ids[:20].tolist()}\")\n",
    "print(f\"First 20 target tokens: {trg_ids[:20].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Dataset & DataLoader\n",
    "\n",
    "Now we create the data pipeline that feeds batches to our model:\n",
    "\n",
    "1. **Dataset**: Wraps our data, tokenizes on-the-fly, returns (source, target) pairs\n",
    "2. **DataLoader**: Batches samples, shuffles for training, handles parallel loading\n",
    "\n",
    "**Key concept**: During training, the decoder gets the target sequence **shifted by one**:\n",
    "- **Decoder input**: `<sos> token1 token2 ... tokenN`\n",
    "- **Decoder target**: `token1 token2 ... tokenN <eos>`\n",
    "\n",
    "This teaches the model to predict the **next token** given previous tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source shape: torch.Size([512])\n",
      "Target shape: torch.Size([64])\n",
      "\n",
      "First 20 source tokens: [5840, 5282, 25, 314, 22979, 220, 14746, 13, 2141, 345, 765, 617, 30, 198, 43462, 25, 10889, 0, 198, 5840]\n",
      "First 20 target tokens: [5840, 5282, 22979, 14746, 290, 481, 2222, 13075, 617, 9439, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SummarizationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for dialogue summarization.\n",
    "    \n",
    "    For each sample, returns:\n",
    "    - src: tokenized dialogue (encoder input)\n",
    "    - trg: tokenized summary (decoder input/output)\n",
    "    \"\"\"\n",
    "    def __init__(self, hf_dataset, tokenizer, src_max_len, trg_max_len):\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.src_max_len = src_max_len\n",
    "        self.trg_max_len = trg_max_len\n",
    "        \n",
    "        # Special token IDs\n",
    "        self.pad_id = tokenizer.pad_token_id\n",
    "        self.eos_id = tokenizer.eos_token_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        dialogue = sample['dialogue']\n",
    "        summary = sample['summary']\n",
    "        \n",
    "        # Tokenize source (dialogue)\n",
    "        src_tokens = self.tokenizer.encode(dialogue)\n",
    "        \n",
    "        # Tokenize target (summary) - add EOS at the end\n",
    "        trg_tokens = self.tokenizer.encode(summary) + [self.eos_id]\n",
    "        \n",
    "        # Truncate if needed\n",
    "        src_tokens = src_tokens[:self.src_max_len]\n",
    "        trg_tokens = trg_tokens[:self.trg_max_len]\n",
    "        \n",
    "        # Pad to fixed length\n",
    "        src_padded = src_tokens + [self.pad_id] * (self.src_max_len - len(src_tokens))\n",
    "        trg_padded = trg_tokens + [self.pad_id] * (self.trg_max_len - len(trg_tokens))\n",
    "        \n",
    "        return {\n",
    "            'src': torch.tensor(src_padded, dtype=torch.long),\n",
    "            'trg': torch.tensor(trg_padded, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Test the dataset\n",
    "test_dataset = SummarizationDataset(dataset, tokenizer, SRC_MAX_LENGTH, TRG_MAX_LENGTH)\n",
    "sample = test_dataset[0]\n",
    "\n",
    "print(f\"Source shape: {sample['src'].shape}\")\n",
    "print(f\"Target shape: {sample['trg'].shape}\")\n",
    "print(f\"\\nFirst 20 source tokens: {sample['src'][:20].tolist()}\")\n",
    "print(f\"First 20 target tokens: {sample['trg'][:20].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1800\n",
      "Validation samples: 200\n"
     ]
    }
   ],
   "source": [
    "# Split into train and validation sets\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Use 90% for training, 10% for validation\n",
    "full_dataset = SummarizationDataset(dataset, tokenizer, SRC_MAX_LENGTH, TRG_MAX_LENGTH)\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches: 28\n",
      "Validation batches: 4\n",
      "\n",
      "Batch source shape: torch.Size([64, 512])\n",
      "Batch target shape: torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "BATCH_SIZE = 64  # Batch size for learning (increase on GPU with more memory)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,      # Shuffle for training\n",
    "    num_workers=0,     # Set to 0 for Windows compatibility\n",
    "    drop_last=True     # Drop incomplete batches\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,     # No shuffle for validation\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Let's look at one batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"\\nBatch source shape: {batch['src'].shape}\")   # [batch_size, src_max_len]\n",
    "print(f\"Batch target shape: {batch['trg'].shape}\")     # [batch_size, trg_max_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Now we train the model! Key components:\n",
    "\n",
    "1. **Loss Function**: CrossEntropyLoss — compares predicted token probabilities with actual tokens\n",
    "2. **Optimizer**: Adam — adaptive learning rate optimizer\n",
    "3. **Teacher Forcing**: During training, we feed the **ground truth** previous token to predict the next one\n",
    "\n",
    "**The training flow:**\n",
    "```\n",
    "Source (dialogue) → Encoder → Context\n",
    "Target[:-1] (shifted) → Decoder + Context → Predictions\n",
    "Predictions vs Target[1:] → Loss → Backprop → Update weights\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Total parameters: 44,432,465\n",
      "Trainable parameters: 44,432,465\n"
     ]
    }
   ],
   "source": [
    "# Initialize model with our actual vocabulary size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model hyperparameters\n",
    "VOCAB_SIZE = len(tokenizer)  # 50257 for GPT-2\n",
    "PAD_IDX = tokenizer.pad_token_id\n",
    "EMBED_SIZE = 256\n",
    "NUM_LAYERS = 3      # Reduce for faster training (original paper uses 6)\n",
    "HEADS = 8\n",
    "FORWARD_EXPANSION = 4\n",
    "DROPOUT = 0.1\n",
    "MAX_LENGTH = max(SRC_MAX_LENGTH, TRG_MAX_LENGTH)\n",
    "\n",
    "# Create model\n",
    "model = Transformer(\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    target_vocab_size=VOCAB_SIZE,  # Same vocab for src and target\n",
    "    src_pad_idx=PAD_IDX,\n",
    "    target_pad_idx=PAD_IDX,\n",
    "    embed_size=EMBED_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    forward_expansion=FORWARD_EXPANSION,\n",
    "    heads=HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    device=device,\n",
    "    max_length=MAX_LENGTH\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: CrossEntropyLoss (ignoring pad_idx=50256)\n",
      "Optimizer: Adam (lr=0.0005)\n"
     ]
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# CrossEntropyLoss ignores padding tokens when computing loss\n",
    "# Also why this specific loss function?\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)  # What is the need for PAD_IDX?\n",
    "\n",
    "# Adam optimizer with learning rate\n",
    "LEARNING_RATE = 5e-4  # Why this value?\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"Loss function: CrossEntropyLoss (ignoring pad_idx={PAD_IDX})\")\n",
    "print(f\"Optimizer: Adam (lr={LEARNING_RATE})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        src = batch['src'].to(device)    # [batch, src_len]\n",
    "        trg = batch['trg'].to(device)    # [batch, trg_len]\n",
    "        \n",
    "        # Decoder input: all tokens except last (teacher forcing)\n",
    "        # Decoder target: all tokens except first (what we predict)\n",
    "        # ----- Are we teaching the model to predict the next token or the next entire sequence? -----\n",
    "        trg_input = trg[:, :-1]   # [batch, trg_len-1]\n",
    "        trg_target = trg[:, 1:]   # [batch, trg_len-1]\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()  # We reset the gradients to zero as in PyTorch autograd accumulates gradients\n",
    "        output = model(src, trg_input)  # [batch, trg_len-1, vocab_size]\n",
    "        \n",
    "        # Reshape for loss: [batch * seq_len, vocab_size] vs [batch * seq_len]\n",
    "        output = output.reshape(-1, output.shape[-1])\n",
    "        trg_target = trg_target.reshape(-1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, trg_target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (prevents exploding gradients)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()  # Why .item() is needed here?\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined! ✓\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate on validation set\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            src = batch['src'].to(device)\n",
    "            trg = batch['trg'].to(device)\n",
    "            \n",
    "            trg_input = trg[:, :-1]\n",
    "            trg_target = trg[:, 1:]\n",
    "            \n",
    "            output = model(src, trg_input)\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            trg_target = trg_target.reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg_target)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "print(\"Training functions defined! ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 20 epochs...\n",
      "============================================================\n",
      "Epoch  1/20 | Train Loss: 8.5541 | Val Loss: 7.1284 ← Best!\n",
      "Epoch  2/20 | Train Loss: 6.7001 | Val Loss: 7.0484 ← Best!\n",
      "Epoch  3/20 | Train Loss: 6.6062 | Val Loss: 7.0663\n",
      "Epoch  4/20 | Train Loss: 6.5866 | Val Loss: 7.1036\n",
      "Epoch  5/20 | Train Loss: 6.5779 | Val Loss: 7.1214\n",
      "Epoch  6/20 | Train Loss: 6.5736 | Val Loss: 7.1373\n",
      "Epoch  7/20 | Train Loss: 6.5497 | Val Loss: 7.0845\n",
      "Epoch  8/20 | Train Loss: 6.4252 | Val Loss: 6.7760 ← Best!\n",
      "Epoch  9/20 | Train Loss: 6.1566 | Val Loss: 6.5521 ← Best!\n",
      "Epoch 10/20 | Train Loss: 5.8850 | Val Loss: 6.3673 ← Best!\n",
      "Epoch 11/20 | Train Loss: 5.6580 | Val Loss: 6.2703 ← Best!\n",
      "Epoch 12/20 | Train Loss: 5.4775 | Val Loss: 6.2025 ← Best!\n",
      "Epoch 13/20 | Train Loss: 5.3316 | Val Loss: 6.1694 ← Best!\n",
      "Epoch 14/20 | Train Loss: 5.1918 | Val Loss: 6.1307 ← Best!\n",
      "Epoch 15/20 | Train Loss: 5.0639 | Val Loss: 6.1175 ← Best!\n",
      "Epoch 16/20 | Train Loss: 4.9524 | Val Loss: 6.1172 ← Best!\n",
      "Epoch 17/20 | Train Loss: 4.8397 | Val Loss: 6.1221\n",
      "Epoch 18/20 | Train Loss: 4.7406 | Val Loss: 6.1378\n",
      "Epoch 19/20 | Train Loss: 4.6382 | Val Loss: 6.1523\n",
      "Epoch 20/20 | Train Loss: 4.5304 | Val Loss: 6.1845\n",
      "============================================================\n",
      "Training complete! Best validation loss: 6.1172\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "NUM_EPOCHS = 20\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Track best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        marker = \" ← Best!\"\n",
    "    else:\n",
    "        marker = \"\"\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{NUM_EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}{marker}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training complete! Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference (Autoregressive Generation)\n",
    "\n",
    "Now the fun part — generating summaries! \n",
    "\n",
    "**How autoregressive generation works:**\n",
    "1. Encode the source (dialogue) once\n",
    "2. Start with just `<eos>` token (or empty)\n",
    "3. Predict next token, append to sequence\n",
    "4. Repeat until `<eos>` or max length reached\n",
    "\n",
    "```\n",
    "Step 1: [] → predict \"Amanda\" \n",
    "Step 2: [Amanda] → predict \"baked\"\n",
    "Step 3: [Amanda, baked] → predict \"cookies\"\n",
    "...until <eos>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation function defined! ✓\n"
     ]
    }
   ],
   "source": [
    "def generate_summary(model, dialogue, tokenizer, device, max_len=64):\n",
    "    \"\"\"\n",
    "    Generate a summary for a given dialogue using autoregressive decoding.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Transformer model\n",
    "        dialogue: Input dialogue string\n",
    "        tokenizer: Tokenizer\n",
    "        device: cuda or cpu\n",
    "        max_len: Maximum tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        Generated summary string\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize source\n",
    "    src_tokens = tokenizer.encode(dialogue)\n",
    "    src_tokens = src_tokens[:SRC_MAX_LENGTH]  # Truncate if needed\n",
    "    src_padded = src_tokens + [tokenizer.pad_token_id] * (SRC_MAX_LENGTH - len(src_tokens))\n",
    "    src_tensor = torch.tensor([src_padded], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Start with empty target (we'll build it token by token)\n",
    "    # Using EOS as the starting token\n",
    "    generated = [tokenizer.eos_token_id]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            # Prepare target tensor\n",
    "            trg_tensor = torch.tensor([generated], dtype=torch.long).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src_tensor, trg_tensor)  # [1, seq_len, vocab_size]\n",
    "            \n",
    "            # Get the last token prediction\n",
    "            next_token_logits = output[0, -1, :]  # [vocab_size]\n",
    "            \n",
    "            # Greedy decoding: pick the token with highest probability\n",
    "            next_token = next_token_logits.argmax().item()\n",
    "            \n",
    "            # Stop if EOS token\n",
    "            if next_token == tokenizer.eos_token_id:\n",
    "                break\n",
    "            \n",
    "            # Append to generated sequence\n",
    "            generated.append(next_token)\n",
    "    \n",
    "    # Decode tokens to text (skip the initial EOS token)\n",
    "    summary = tokenizer.decode(generated[1:])\n",
    "    return summary\n",
    "\n",
    "print(\"Generation function defined! ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING ON TRAINING EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "--- Example 0 ---\n",
      "DIALOGUE:\n",
      "Amanda: I baked  cookies. Do you want some?\n",
      "Jerry: Sure!\n",
      "Amanda: I'll bring you tomorrow :-)\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "Amanda baked cookies and will bring Jerry some tomorrow.\n",
      "\n",
      "GENERATED SUMMARY:\n",
      " is going to buy a new job. She will come to go to the weekend.                                               a\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "--- Example 5 ---\n",
      "DIALOGUE:\n",
      "Neville: Hi there, does anyone remember what date I got married on?\n",
      "Don: Are you serious?\n",
      "Neville: Dead serious. We're on vacation, and Tina's mad at me about something. I have a strange suspicion tha...\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "Wyatt reminds Neville his wedding anniversary is on the 17th of September. Neville's wife is upset and it might be because Neville forgot about their anniversary.\n",
      "\n",
      "GENERATED SUMMARY:\n",
      " is going to buy a new job. She will come to go to the weekend.                                               a\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "--- Example 10 ---\n",
      "DIALOGUE:\n",
      "Lucas: Hey! How was your day?\n",
      "Demi: Hey there! \n",
      "Demi: It was pretty fine, actually, thank you!\n",
      "Demi: I just got promoted! :D\n",
      "Lucas: Whoa! Great news!\n",
      "Lucas: Congratulations!\n",
      "Lucas: Such a success has ...\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "Demi got promoted. She will celebrate that with Lucas at Death & Co at 10 pm.\n",
      "\n",
      "GENERATED SUMMARY:\n",
      " is going to buy a new job. She will come to go to the weekend.                                               a\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test on a few examples from the dataset\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING ON TRAINING EXAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in [0, 5, 10]:\n",
    "    sample = dataset[i]\n",
    "    dialogue = sample['dialogue']\n",
    "    actual_summary = sample['summary']\n",
    "    \n",
    "    # Generate summary\n",
    "    generated_summary = generate_summary(model, dialogue, tokenizer, device)\n",
    "    \n",
    "    print(f\"\\n--- Example {i} ---\")\n",
    "    print(f\"DIALOGUE:\\n{dialogue[:200]}...\" if len(dialogue) > 200 else f\"DIALOGUE:\\n{dialogue}\")\n",
    "    print(f\"\\nACTUAL SUMMARY:\\n{actual_summary}\")\n",
    "    print(f\"\\nGENERATED SUMMARY:\\n{generated_summary}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING ON NEW DIALOGUE\n",
      "======================================================================\n",
      "DIALOGUE:\n",
      "\n",
      "John: Hey, are you coming to the party tonight?\n",
      "Sarah: What party?\n",
      "John: Mike's birthday party at 8pm!\n",
      "Sarah: Oh I totally forgot! Where is it?\n",
      "John: At his place. I can pick you up if you want.\n",
      "Sarah: That would be great! Thanks!\n",
      "John: No problem. See you at 7:30.\n",
      "\n",
      "\n",
      "GENERATED SUMMARY:\n",
      " is going to buy a new job. She will come to go to the weekend.                                               a\n"
     ]
    }
   ],
   "source": [
    "# Try with a completely NEW dialogue!\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING ON NEW DIALOGUE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "new_dialogue = \"\"\"\n",
    "John: Hey, are you coming to the party tonight?\n",
    "Sarah: What party?\n",
    "John: Mike's birthday party at 8pm!\n",
    "Sarah: Oh I totally forgot! Where is it?\n",
    "John: At his place. I can pick you up if you want.\n",
    "Sarah: That would be great! Thanks!\n",
    "John: No problem. See you at 7:30.\n",
    "\"\"\"\n",
    "\n",
    "generated = generate_summary(model, new_dialogue, tokenizer, device)\n",
    "\n",
    "print(f\"DIALOGUE:\\n{new_dialogue}\")\n",
    "print(f\"\\nGENERATED SUMMARY:\\n{generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
