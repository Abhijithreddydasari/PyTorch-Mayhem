{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()  # Python 2.0 & above applicable\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim =  embed_size // heads\n",
    "\n",
    "        assert(self.head_dim * heads == embed_size)\n",
    "\n",
    "        # Where are we using these ones in the `forward function`?\n",
    "        self.queries = nn.Linear(self.embed_size, self.embed_size, bias=False)\n",
    "        self.keys = nn.Linear(self.embed_size, self.embed_size, bias=False)\n",
    "        self.values = nn.Linear(self.embed_size, self.embed_size, bias=False)\n",
    "\n",
    "        self.fc_out = nn.Linear(self.heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, query, keys, values, mask: bool):\n",
    "        N = query.shape[0]  # Training batch size or number of batches?\n",
    "        query_len, key_len, value_len = query.shape[1], keys.shape[1], values.shape[1]\n",
    "\n",
    "        queries_proj = self.queries(query)  # (N, Seq, embed_size)\n",
    "        keys_proj = self.keys(keys)\n",
    "        values_proj = self.values(values)\n",
    "\n",
    "        # Split embedding into self.heads pieces\n",
    "        queries = queries_proj.reshape(N, query_len, self.heads, self.head_dim)  # What is the difference between key_len and N in the first place?\n",
    "        keys = keys_proj.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        values = values_proj.reshape(N, value_len, self.heads, self.head_dim)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd, nkhd->nhqk\", [queries, keys])  # Why an array? & Why this arrangement?\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        \n",
    "        attention = torch.softmax(energy/ self.embed_size ** (1/2), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhqk,nkhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )  # The shape of the whole must be looked at & how does it reshape this (are the vectors going to the correct place)??\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        \"\"\"\n",
    "        The forward expansion term basically takes the vector from a lower dimensional space to a \n",
    "        higher dimensional space for better representation capacity. The value used in the paper is\n",
    "        equal to 4.\n",
    "        \"\"\"\n",
    "        super().__init__()  # Python 3.0 & above applicable\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)  # Why embedding size?\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        attention = self.attention(query, key, value, mask) # How is this calling the forward method?\n",
    "\n",
    "        x = self.dropout(self.norm1(attention + query))  # Adding Skip Connections\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(  # Why isn't any input in the initialization? How is the input fed to the Encoder?\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length  # Maximum possible length of the sentence\n",
    "    ):  # The above parameters are hyperparameters for the model that we are training\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size  # Do we really need another instance variable embed_size??\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)  # Is this good enough for big models?\n",
    "        self.positional_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_size, heads, dropout, forward_expansion) for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)  # Why do we need this again? We don't seem to be using this in `forward`\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)  # What does .expand() do here?\n",
    "        out = self.word_embedding(x) + self.positional_embedding(positions)  # Adding or concatenation?\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)  # What is the use of having separate matrices defined when we pass the same thing here?\n",
    "            # Also why this? Can't we directly use the TransformerBlock instead of self.layers?\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, target_mask):  # Read about src and target masks\n",
    "        attention = self.attention(x, x, x, target_mask)\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(query, key, value, src_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(target_vocab_size, embed_size)  # If we get the value embeddings from the previous layer, do we need this? (for seq2seq) - is my question sensible in the first place?\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device) for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, target_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, target_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))  # Why dropout here?\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, target_mask)  # Why are we giving the same values here when we have separate matrices for keys and values?\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        target_vocab_size,\n",
    "        src_pad_idx,  # Necessary to compute mask we are going to use\n",
    "        target_pad_idx,\n",
    "        embed_size = 256,\n",
    "        num_layers = 6,\n",
    "        forward_expansion = 4,\n",
    "        heads = 8,\n",
    "        dropout = 0,\n",
    "        device = \"cuda\",\n",
    "        max_length = 100\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            target_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length  # What is the purpose of this variable??\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.target_pad_idx = target_pad_idx\n",
    "        self.device =  device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)  # Look into the .unsqueeze() function\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_target_mask(self, target):\n",
    "        N, target_len = target.shape\n",
    "        target_mask =  torch.tril(torch.ones((target_len, target_len))).expand(  # Triangular lower for masking\n",
    "            N, 1, target_len, target_len  # To expand it to the training examples I guess?\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, src, target):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        target_mask = self.make_target_mask(target)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(target, enc_src, src_mask, target_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "tensor([[[-0.4076, -0.8250, -0.3485,  0.7102,  0.1573, -0.4691, -0.0984,\n",
      "           0.3193,  0.6115, -0.0237],\n",
      "         [ 0.0092,  0.0964,  0.8867, -0.3189,  0.2491, -1.0998, -0.0293,\n",
      "           0.2697, -0.0599,  0.0719],\n",
      "         [-0.9265, -0.1103,  0.8022, -0.3058,  0.1895, -0.5535, -0.1163,\n",
      "           0.1099,  1.0126,  0.4507],\n",
      "         [-0.1720,  0.1499, -0.0063, -0.1559, -0.1430, -0.9666,  0.3372,\n",
      "           0.8199, -0.4038,  0.0190],\n",
      "         [ 0.4433, -0.7454,  0.0722,  0.2000, -0.8239, -0.4571, -0.2999,\n",
      "           0.6386, -0.1653,  0.5347],\n",
      "         [ 0.9607, -0.2463,  0.6453,  0.2526, -0.2535, -0.6740,  0.0807,\n",
      "           0.2469,  0.1038,  0.3005],\n",
      "         [-0.5002, -0.8179,  0.3131,  0.4771, -0.4466, -0.6380, -0.1282,\n",
      "           1.9126,  0.0919,  0.5859]],\n",
      "\n",
      "        [[-0.3257, -0.8137, -0.2222,  0.7361,  0.3636, -0.4141, -0.3308,\n",
      "           0.3201,  0.4566, -0.0326],\n",
      "         [ 0.3360, -0.5969,  0.3104,  0.1160, -0.8658, -0.2600, -0.4460,\n",
      "           0.6846,  0.3175, -0.3218],\n",
      "         [-0.3420, -0.4764,  1.3700, -0.7211,  0.1625,  0.1479, -0.7581,\n",
      "           0.7953, -0.0138,  0.7001],\n",
      "         [-0.1283, -0.7332,  0.4019,  0.7083, -0.7488, -0.6903, -0.0814,\n",
      "           0.8177,  0.0195, -0.1717],\n",
      "         [-0.1542, -0.0667,  0.2552,  0.0395,  0.0352, -0.9088, -0.4086,\n",
      "           0.6036,  0.2740,  1.1254],\n",
      "         [ 0.4119,  0.1666,  0.6168, -0.1964,  0.6422, -0.8320,  0.1998,\n",
      "           0.3844, -0.5172,  0.6072],\n",
      "         [-0.1891, -0.5778,  0.8070, -0.8954,  0.3354,  0.0788, -0.0942,\n",
      "           2.2690, -0.7423,  1.1429]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([2, 7, 10])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(\n",
    "    device\n",
    ")\n",
    "trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
    "\n",
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "src_vocab_size = 10\n",
    "trg_vocab_size = 10\n",
    "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(\n",
    "    device\n",
    ")\n",
    "out = model(x, trg[:, :-1])\n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
